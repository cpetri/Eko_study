{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KNNImputer' from 'sklearn.impute' (C:\\Users\\Camil\\anaconda3\\lib\\site-packages\\sklearn\\impute.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-60cfc9ab4fba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KNNImputer' from 'sklearn.impute' (C:\\Users\\Camil\\anaconda3\\lib\\site-packages\\sklearn\\impute.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import KNNImputer\n",
    "import os\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def CV_train_stratified_lr(X_train,y_train, weights, l2_coef, interactions):\n",
    "    \"\"\"Function to train and assess through 5 folds cross validation a logistic regression model with different parameters.\n",
    "    X_train : Matrix of features\n",
    "    y_train : Vector of labels\n",
    "    weights : Vector of weights used for the weighted loss function\n",
    "    l2_coef : Vector of coefficient for ridge regularisation\n",
    "    interactions : Add polynomial features to take into account interaction between variables in X_train.\n",
    "    The metric used for assessment is F1 score.\n",
    "    \n",
    "    returns a list of F1 scores for each models, which are the average of F1 scores on each cross validation fold.\n",
    "    \"\"\"\n",
    "    #if interactions != int and interactions != None:\n",
    "        #print(\"interactions must be of type int or None\")\n",
    "    \n",
    "    if type(interactions) == int:\n",
    "        \n",
    "        poly = PolynomialFeatures(interactions, interaction_only=True)\n",
    "        X_train = poly.fit_transform(X_train)\n",
    "    else:\n",
    "        pass\n",
    "    cv_res_list = []\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for i in l2_coef:\n",
    "        for j in weights:\n",
    "                \n",
    "                \n",
    "            results = []\n",
    "            for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "                train_X, test_X = X_train[train_ix], X_train[test_ix]\n",
    "                train_y, test_y = y_train[train_ix], y_train[test_ix]\n",
    "                clf = LogisticRegression(class_weight= j , penalty = \"l2\", C = i , random_state=Random_seed).fit(train_X, train_y.ravel())\n",
    "                res = f1_score(test_y,clf.predict(test_X))\n",
    "                results.append(res)\n",
    "            mean = sum(results)/len(results)    \n",
    "            cv_res_list.append([i,j,mean])\n",
    "    return(cv_res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_train_stratified_xgb(X_train,y_train, lr, n_estimator,reg_lambda ,max_depth):\n",
    "    \"\"\"Function to train and assess through 5 folds cross validation a gradient boosting classification model with different parameters.\n",
    "    The metric used for assessment is F1 score.\n",
    "    X_train : Matrix of features\n",
    "    y_train : Vector of labels\n",
    "    lr : Vector of learning rates\n",
    "    n_estimators : Vector of amount of weak models\n",
    "    reg_lambda : Vector of values for L2 regularisation\n",
    "    max_depth : Maximum depth of weak learners \n",
    "    l2_coef : vector of coefficient for ridge regularisation\n",
    "    interactions : add polynomial features to take into account interaction between variables in X_train. \n",
    "    \n",
    "    returns a list of F1 scores for each models, which are the average of F1 scores on each cross validation fold.\n",
    "    \"\"\"\n",
    "    cv_res_list = []\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pos_weight = (len(y_train) - np.sum(y_train))/np.sum(y_train)\n",
    "    #pos_weight = 20\n",
    "    for i in lr:\n",
    "        for j in n_estimator:\n",
    "            for k in reg_lambda:\n",
    "                for l in max_depth:\n",
    "                    \n",
    "                    results = []\n",
    "                    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "                        train_X, test_X = X_train[train_ix], X_train[test_ix]\n",
    "                        train_y, test_y = y_train[train_ix], y_train[test_ix]\n",
    "                        clf = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, n_estimators = j, max_depth = l, learning_rate = i, scale_pos_weight = pos_weight, reg_lambda = k)\n",
    "                        clf.fit(train_X, train_y.ravel())\n",
    "                        res = f1_score(test_y,clf.predict(test_X))\n",
    "                        results.append(res)\n",
    "                    mean = sum(results)/len(results)    \n",
    "                    cv_res_list.append([i,j,k,l,mean])\n",
    "    return(cv_res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_train_stratified_randomforest(X_train,y_train, n_estimator,min_samples_leaf,max_features ,max_depth, criterion = \"gini\"):\n",
    "    \"\"\"Function to train and assess through 5 folds cross validation a random forest classification model with different parameters.\n",
    "    The metric used for assessment is F1 score.\n",
    "    X_train : Matrix of features\n",
    "    y_train : Vector of labels\n",
    "    n_estimators : Vector of amount of trees\n",
    "    min_samples_leaf : Vector the minimum number of samples required to be at a leaf node\n",
    "    max_features : Vector of the number of features to consider when looking for the best split\n",
    "    max_depth : Maximum depth of trees \n",
    "    l2_coef : vector of coefficient for ridge regularisation\n",
    "    interactions : add polynomial features to take into account interaction between variables in X_train. \n",
    "    \n",
    "    returns a list of F1 scores for each models, which are the average of F1 scores on each cross validation fold.\n",
    "    \"\"\"\n",
    "    cv_res_list = []\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pos_weight = (len(y_train) - np.sum(y_train))/np.sum(y_train)\n",
    "    #pos_weight = 20\n",
    "    for i in n_estimator:\n",
    "        for j in min_samples_leaf:\n",
    "            for k in max_features:\n",
    "                for l in max_depth:\n",
    "                    \n",
    "                    results = []\n",
    "                    for train_ix, test_ix in kfold.split(X_train, y_train):\n",
    "                        train_X, test_X = X_train[train_ix], X_train[test_ix]\n",
    "                        train_y, test_y = y_train[train_ix], y_train[test_ix]\n",
    "                        clf = RandomForestClassifier(n_estimators = i, min_sample_leaf = j, max_features = k, max_depth = l)\n",
    "                        clf.fit(train_X, train_y.ravel())\n",
    "                        res = f1_score(test_y,clf.predict(test_X))\n",
    "                        results.append(res)\n",
    "                    mean = sum(results)/len(results)    \n",
    "                    cv_res_list.append([i,j,k,l,mean])\n",
    "    return(cv_res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC_and_Perf(target, pred_probs, label,title, save):\n",
    "    \"\"\"Function to return the ROC curve, AUC and different relevant performance metrics for a model.\n",
    "    \n",
    "    target : vector of true labels\n",
    "    pred_probs : Vector of model outputs \\in [0,1]\n",
    "    label : ROC curve label on plot\n",
    "    title : Plot title\n",
    "    save : To save the plot\n",
    "  \n",
    "    \n",
    "    returns a dataframe with for each threshold : Sensitivity, Specificity, PPV, NPV, Accuracy, F1 score, Diagnostic odd ratio (DOR) and DOR confidence intervals.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target, pred_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot([0,1], [0,1], linestyle='--', label='Random classifier')\n",
    "    pyplot.plot(fpr, tpr, marker='.', label= label + str(\" (AUC = %0.2f)\") % roc_auc)\n",
    "    # axis labels\n",
    "    pyplot.xlabel('1 - Specificity')\n",
    "    pyplot.ylabel('Sensitivity')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.title(title)\n",
    "    if save == True:\n",
    "        plt.savefig('ROC_curve' +\" \" + str(title) + '.png')\n",
    "    pyplot.show()\n",
    "    F1s = [] \n",
    "    ppvs = []\n",
    "    npvs = []\n",
    "    DORs = []\n",
    "    ci_up_DORs =[]\n",
    "    ci_low_DORs = []\n",
    "    accuracies = []\n",
    "    for i in thresholds:\n",
    "        preds = (pred_probs>i)*1\n",
    "        #print(preds)\n",
    "        #print(target)\n",
    "        print()\n",
    "        tp = np.sum(((preds==1) & (target==1))*1)\n",
    "        fp = np.sum(((preds==1) & (target==0))*1)\n",
    "        tn = np.sum(((preds==0) & (target==0))*1)\n",
    "        fn = np.sum(((preds==0) & (target ==1))*1)\n",
    "        ppv = tp/(tp + fp)\n",
    "        npv = tn/(tn + fn)\n",
    "        accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "        F1 = tp/( tp + 0.5 * (fp + fn))\n",
    "        DOR = (tp * tn)/(fp * fn)\n",
    "        lnDOR = np.log(DOR)\n",
    "        SE_lnDOR = np.sqrt(1/tp + 1/fn + 1/fp + 1/tn)\n",
    "        ci_up_DOR = np.exp(lnDOR + 1.96 * SE_lnDOR)\n",
    "        ci_low_DOR = np.exp(lnDOR - 1.96 * SE_lnDOR)\n",
    "        ci_up_DORs.append(ci_up_DOR)\n",
    "        ci_low_DORs.append(ci_low_DOR)\n",
    "        DORs.append(DOR)\n",
    "        F1s.append(F1)\n",
    "        ppvs.append(ppv)\n",
    "        npvs.append(npv)\n",
    "        accuracies.append(accuracy)\n",
    "    perf_data = pd.DataFrame(list(zip(thresholds, tpr, 1-fpr,ppvs,npvs,accuracies,F1s,DORs,ci_up_DORs,ci_low_DORs)),\n",
    "                   columns =['Threshold', 'Sensitivity',\"Specificity\",\"PPV\",\"NPV\", \"Accuracy\", \"F1 score\",\"DOR\",\"CI_up\",\"CI_low\"])\n",
    "    return perf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold(data, min_se = None, min_sp = None):\n",
    "      \"\"\"Function to return the best threshold with a minimum sensitivity or specificity.\n",
    "    \n",
    "    data : a dataframe with \"Sensitivity\" and \"Specificity\" column names.\n",
    "    min_se : minimum wanted sensitivity\n",
    "    min_sp : minimum wanted specificity\n",
    "  \n",
    "    \n",
    "    returns a threshold value that maximises sensitivity and specificity with the wanted conditions\n",
    "    \"\"\"\n",
    "    if min_se == None:\n",
    "         res = data.loc[data[['Sensitivity','Specificity']].sum(1).idxmax()]\n",
    "    else:\n",
    "        data = data.loc[(data.Sensitivity>=  min_se) & (data.Specificity>= min_sp)]\n",
    "        res = data.loc[data[['Sensitivity','Specificity']].sum(1).idxmax()]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_roc_AUC_CI(pred_probs1,pred_probs2, targets):\n",
    "    \"\"\"Function to return the ROC curve, AUC and different relevant performance metrics for the rule based model combining 2 positions.\n",
    "    Also computes ROC curve of AUC by bootstrap.\n",
    "    targets : vector of true labels\n",
    "    pred_probs1 : Vector of position1 model outputs \\in [0,1]\n",
    "    pred_probs2 : Vector of position2 model outputs \\in [0,1]\n",
    "    \n",
    "  \n",
    "    \n",
    "    returns a dataframe with for each threshold : ROC curves, AUC and boostrapped confidence intervals.\n",
    "    \"\"\"\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    dframe = pd.concat([pred_probs1,pred_probs2,targets], axis = 1)\n",
    "    dframe = dframe.dropna(axis = \"rows\")\n",
    "    posA = dframe.columns[0]\n",
    "    posB=dframe.columns[1]\n",
    "    #targets = dframe.iloc[:,[2]]\n",
    "    n_bootstraps = 10000\n",
    "    rng_seed = 42  # control reproducibility\n",
    "    bootstrapped_scores = []\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        sensitivities = []\n",
    "        specificities = []\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(dframe), len(dframe))\n",
    "        df = dframe.iloc[indices,:]\n",
    "        for t in np.linspace(0,1,101):\n",
    "            targets = df.iloc[:,2]\n",
    "            preds1 = (df.iloc[:,0]>t)*1\n",
    "            preds2 = (df.iloc[:,1]>t)*1\n",
    "            preds = preds1 + preds2\n",
    "            preds = np.where(preds==2,1,preds)\n",
    "            tp = np.sum(((preds==1) & (targets==1))*1)\n",
    "            fp = np.sum(((preds==1) & (targets==0))*1)\n",
    "            tn = np.sum(((preds==0) & (targets==0))*1)\n",
    "            fn = np.sum(((preds==0) & (targets ==1))*1)\n",
    "            sensitivity = tp/(tp + fn)\n",
    "            specificity = tn/(tn + fp)\n",
    "            sensitivities.append(sensitivity)\n",
    "            specificities.append(specificity)\n",
    "        tpr = np.array(sensitivities) \n",
    "        fpr = 1 - np.array(specificities)\n",
    "        score = metrics.auc(fpr, tpr)\n",
    "        bootstrapped_scores.append(score)       \n",
    "    return bootstrapped_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_perf_thresholds(pred_probs, target):\n",
    "    \"\"\"Function to return the best combination of 2 positions for the rule based approach, for each combination, returns the\n",
    "    threshold associated with the best performance in sensitivity and specificity.\n",
    "    pred_probs : dataframe of probabilities\n",
    "    target : Vector true labels\n",
    "    returns a dataframe with for combination of position, the best performance and associated threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_threshold_df1 = pd.DataFrame()\n",
    "    best_threshold_df2 = pd.DataFrame()\n",
    "    final_df = pd.DataFrame()\n",
    "    comb = combinations(pred_probs.columns,2)\n",
    "    for i in comb:\n",
    "        df = pred_probs.loc[:,i]\n",
    "        df = pd.concat([df,target], axis = 1)\n",
    "        df = df.dropna(axis = \"rows\")\n",
    "        posA = df.columns[0]\n",
    "        posB=df.columns[1]\n",
    "        targets = df.iloc[:,2]\n",
    "        thresh = []\n",
    "        sensitivities = []\n",
    "        specificities = []\n",
    "        ppvs = []\n",
    "        npvs = []\n",
    "        accuracies = []\n",
    "        for t in np.linspace(0,1,101):\n",
    "            preds1 = (df.iloc[:,0]>t)*1\n",
    "            preds2 = (df.iloc[:,1]>t)*1\n",
    "            preds = preds1 + preds2\n",
    "            preds = np.where(preds==2,1,preds)\n",
    "            tp = np.sum(((preds==1) & (targets==1))*1)\n",
    "            fp = np.sum(((preds==1) & (targets==0))*1)\n",
    "            tn = np.sum(((preds==0) & (targets==0))*1)\n",
    "            fn = np.sum(((preds==0) & (targets ==1))*1)\n",
    "            sensitivity = tp/(tp + fn)\n",
    "            specificity = tn/(tn + fp)\n",
    "            ppv = tp/(tp + fp)\n",
    "            npv = tn/(tn + fn)\n",
    "            accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "            thresh.append(t)\n",
    "            sensitivities.append(sensitivity)\n",
    "            specificities.append(specificity)\n",
    "            ppvs.append(ppv)\n",
    "            npvs.append(npv)\n",
    "            accuracies.append(accuracy)\n",
    "        perf_data = pd.DataFrame(list(zip(thresh, sensitivities, specificities,ppvs,npvs,accuracies)),\n",
    "            columns =[str(posA) + \" or \" + str(posB) +' Threshold' +\", \" + \"N = \"+str(len(targets)),str(posA) + \" or \" + str(posB) +' Sensitivity',str(posA) + \" or \" + str(posB) + \" Specificity\",str(posA) + \" or \" + str(posB) + \" PPV\", str(posA) + \" or \" + str(posB) + \" NPV\", str(posA) + \" or \" + str(posB) + \" Accuracy\" ])\n",
    "        best_thresh1 = perf_data.loc[perf_data.iloc[:,[1,2]].sum(1).idxmax()].to_frame().transpose().reset_index(drop=True)\n",
    "        #print(best_thresh1)\n",
    "        try:\n",
    "            data_thresh = perf_data[(perf_data.iloc[:,1]>=  0.8) & (perf_data.iloc[:,2]>= 0.7)]\n",
    "            best_thresh2 = data_thresh.loc[data_thresh.iloc[:,[1,2]].sum(1).idxmax()].to_frame().transpose().reset_index(drop=True)\n",
    "        except:\n",
    "            try:\n",
    "                data_thresh = perf_data[(perf_data.iloc[:,1]>=  0.8) & (perf_data.iloc[:,2]>= 0.6)]\n",
    "                best_thresh2 = data_thresh.loc[data_thresh.iloc[:,[1,2]].sum(1).idxmax()].to_frame().transpose().reset_index(drop=True)\n",
    "            except:\n",
    "                try: \n",
    "                    data_thresh = perf_data[(perf_data.iloc[:,1]>=  0.8) & (perf_data.iloc[:,2]>= 0.5)]\n",
    "                    best_thresh2 = data_thresh.loc[data_thresh.iloc[:,[1,2]].sum(1).idxmax()].to_frame().transpose().reset_index(drop=True)\n",
    "                except:\n",
    "                    try:\n",
    "                        data_thresh = perf_data[(perf_data.iloc[:,1]>=  0.8) & (perf_data.iloc[:,2]>= 0.4)]\n",
    "                        best_thresh2 = data_thresh.loc[data_thresh.iloc[:,[1,2]].sum(1).idxmax()].to_frame().transpose().reset_index(drop=True)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "        \n",
    "        best_threshold_df1 = pd.concat([best_threshold_df1,best_thresh1],axis = 1)\n",
    "        print(best_threshold_df1)\n",
    "        best_threshold_df2 = pd.concat([best_threshold_df2,best_thresh2],axis = 1)\n",
    "        final_df = pd.concat([final_df,perf_data], axis = 1)\n",
    "    #final_df = final_df.iloc[:,0:(int(final_df.shape[1]/2))]        \n",
    "    return best_threshold_df1, best_threshold_df2, final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
